{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling and Undersampling\n",
    "\n",
    "Oversampling methods duplicate or create new synthetic examples in the minority class,whereas undersampling methods delete examples in the majority class.  Both types of sampling can be effective when used in isolation, although can be more effective when both types of methods are used together. \n",
    "\n",
    "  After completing this tutorial, you will know:\n",
    "  \n",
    "- How to define a sequence of oversampling and undersampling methods to be applied to a training dataset or when evaluating a classifier model.\n",
    "\n",
    "- How  to  manually  combine  oversampling  and  undersampling  methods  for  imbalanced classification.\n",
    "\n",
    "- How to use pre-defined and well-performing combinations of sampling methods for imbalanced classification.\n",
    "\n",
    "#### Binary Test Problem and Decision Tree Model (baseline on imbalance dataset)\n",
    " \n",
    " Tying this together, the complete example of creating an imbalanced classification dataset and plotting the examples is listed below.\n",
    " \n",
    " Running the example reports the average ROC AUC for the decision tree on the datasetover three repeats of 10-fold cross-validation (e.g.  average over 30 different model evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC AUC: 0.771\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n",
    "# define model\n",
    "model = DecisionTreeClassifier()\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a test problem, model, and test harness, let’s look at manual combinations of oversampling and undersampling methods.\n",
    "\n",
    "The imbalanced-learn Python library provides a range of sampling techniques, as well as aPipelineclass that can be used to create a combined sequence of sampling methods to apply toa dataset.  We can use the Pipeline to construct a sequence of oversampling and undersampling techniques to apply to a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC AUC: 0.814\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n",
    "# define model\n",
    "model = DecisionTreeClassifier()\n",
    "# define sampling\n",
    "over = RandomOverSampler(sampling_strategy=0.1)\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "# define pipeline\n",
    "pipeline = Pipeline(steps=[('o', over), ('u', under), ('m', model)])\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE and Random Undersampling\n",
    "\n",
    "Perhaps the most popular oversampling method is the Synthetic Minority Oversampling Technique, or SMOTE for short.  SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample as a point along that line.  The authors of the technique recommend using SMOTE on the minority class, followed by an undersampling technique on the majority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC AUC: 0.843\n"
     ]
    }
   ],
   "source": [
    "# combination of SMOTE and random undersampling for imbalanced classification\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n",
    "# define model\n",
    "model = DecisionTreeClassifier()\n",
    "# define pipeline\n",
    "over = SMOTE(sampling_strategy=0.1)\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "steps = [('o', over), ('u', under), ('m', model)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Combined Data Sampling Methods\n",
    "\n",
    "There are combinations of oversampling and under sampling methods that have proven effective and  together  may  be  considered  sampling  techniques.   Two  examples  are  the  combination of SMOTE with Tomek Links undersampling and SMOTE with Edited Nearest Neighbors undersampling\n",
    "\n",
    "\n",
    "#### SMOTE and Tomek Links Undersampling\n",
    "\n",
    "SMOTE is an oversampling method that synthesizes new plausible examples in the minority class.  Tomek Links refers to a method for identifying pairs of nearest neighbors in a dataset that have different classes.  Removing one or both of the examples in these pairs (such as the examples in the majority class) has the effect of making the decision boundary in the training dataset less noisy or ambiguous.\n",
    "\n",
    "Gustavo Batista, et al.  tested combining these methods in their 2003 paper titled Balancing Training Data for Automated Annotation of Keywords:  A Case Study.  Specifically, first the SMOTE method is applied to oversample the minority class to a balanced distribution, then examples in Tomek Links from the majority classes are identified and removed.\n",
    "\n",
    "The combination was shown to provide a reduction in false negatives at the cost of an increase in false positives for a binary classification task.\n",
    "\n",
    "The SMOTE configuration can be set via the smote argument and takes a configured SMOTE instance.   The Tomek  Links  configuration  can  be  set  via  the tomek argument  and  takes  a configured TomekLinks object.\n",
    "\n",
    "The default is to balance the dataset with SMOTE then remove Tomek links from all classes.  This is the approach used in another paper that explores this combination entitled A Study of the Behavior of Several Methods for Balancing Machine Learning TrainingData.\n",
    "\n",
    "Alternately, we can configure the combination to only remove links from the majority class as described in the 2003 paper by specifying the tomek argument with an instance of TomekLinks with the sampling strategy argument set to only undersample the ‘majority’ class.\n",
    "\n",
    "In this case, it seems that this combined sampling strategy does not offer a benefit for thismodel on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC AUC: 0.815\n"
     ]
    }
   ],
   "source": [
    "# combined SMOTE and Tomek Links sampling for imbalanced classification\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n",
    "# define model\n",
    "model = DecisionTreeClassifier()\n",
    "# define sampling\n",
    "resample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
    "# define pipeline\n",
    "pipeline = Pipeline(steps=[('r', resample), ('m', model)])\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE and Edited Nearest Neighbors Undersampling\n",
    "\n",
    "SMOTE may be the most popular oversampling technique and can be combined with manydifferent undersampling techniques.  Another very popular undersampling method is the EditedNearest Neighbors, or ENN, rule.  This rule involves usingk= 3 nearest neighbors to locatethose examples in a dataset that are misclassified and that are then removed. \n",
    "Gustavo Batista, et al.  explore many combinations of oversampling and undersampling methods compared to the methods used inisolation in their 2004 paper titled A Study of the Behavior of Several Methods for BalancingMachine Learning Training Data.  This includes the combinations:\n",
    "\n",
    "- Condensed Nearest Neighbors + Tomek Links\n",
    "- SMOTE + Tomek Links\n",
    "- SMOTE + Edited Nearest Neighbors\n",
    "\n",
    "Regarding this final combination, the authors comment that ENN is more aggressive at downsampling the majority class than Tomek Links, providing more in-depth cleaning.  They apply the method, removing examples from both the majority and minority classes\n",
    "\n",
    "*ENN is used to remove examples from both classes.  Thus, any example that is misclassified by its three nearest neighbors is removed from the training set.*\n",
    "— A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data,2004\n",
    "\n",
    "The SMOTE configuration can be set as a SMOTE object via the smote argument, and the ENN configuration can be set via the EditedNearestNeighbours object via the enn argument.SMOTE  defaults  to  balancing  the  distribution,  followed  by  ENN  that  by  default  removes misclassified examples from all classes.  We could change the ENN to only remove examples from the majority class by setting the enn argument to an EditedNearestNeighbours instance with sampling strategy argument set to ‘majority’.\n",
    "\n",
    "\n",
    "In this case, we see a further lift in performance over SMOTE with the random under sampling method from about 0.81 to about 0.85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC AUC: 0.849\n"
     ]
    }
   ],
   "source": [
    "# combined SMOTE and Edited Nearest Neighbors sampling for imbalanced classification\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.combine import SMOTEENN\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n",
    "# define model\n",
    "model = DecisionTreeClassifier()\n",
    "# define sampling\n",
    "resample = SMOTEENN()\n",
    "# define pipeline\n",
    "pipeline = Pipeline(steps=[('r', resample), ('m', model)])\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result highlights that editing the oversampled minority class may also be an important consideration that could easily be overlooked.  \n",
    "\n",
    "This was the same finding in the 2004 paper where the authors discover that SMOTE with Tomek Links and SMOTE with ENN perform well across a range of datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
