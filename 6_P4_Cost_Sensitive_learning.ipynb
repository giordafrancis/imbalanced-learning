{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost sensitive learning\n",
    "\n",
    "Cost-sensitive learning is a subfield of machine learning that takes the costs of prediction errors (and potentially other costs) into account when training a machine learning model.  It is a field of study that is closely related to the field of imbalanced learning that is concerned with classification on datasets with a skewed class distribution.\n",
    "\n",
    "This subfield of machine learning that is focused on learning and using models on data that have uneven penalties or costs when making predictions and more.\n",
    "\n",
    "\n",
    "####  Not All Classification Errors Are Equal\n",
    "\n",
    " Most  machine  learning  algorithms  designed  forclassification assume that there is an equal number of examples for each observed class.  This isnot always the case in practice, and datasets that have a skewed class distribution are referredto as imbalanced classification problems.\n",
    " \n",
    "In  addition  to  assuming  that  the  class  distribution  is  balanced,  most  machine  learning algorithms also assume that the prediction errors made by a classifier are the same, so-calledmiss-classifications.  This is typically not the case for binary classification problems, especially those that have an imbalanced class distribution. \n",
    "\n",
    "Machine learning algorithms that treat each type of misclassification error as the same are unable to meet the needs of these types of problems.  As such, both the underrepresentation of the minority class in the training data and the increased importance on correctly identifying examples from the minority class make imbalanced classification one of the most challenging problems in applied machine learning.\n",
    "\n",
    "Traditionally, machine learning algorithms are trained on a dataset and seek to minimizeerror.  Fitting a model on data solves an optimization problem where we explicitly seek tominimize error.  A range of functions can be used to calculate the error of a model on trainingdata, and the more general term is referred to as loss.  We seek to minimize the loss of a modelon the training data, which is the same as talking about error minimization.\n",
    "\n",
    "\n",
    "- Error Minimization:  The conventional goal when training a machine learning algorithm is to minimize the error of the model on a training dataset\n",
    "\n",
    "- Cost:  The penalty associated with an incorrect prediction.\n",
    "\n",
    "- Cost Minimization:  The goal of cost-sensitive learning is to minimize the cost of a model on a training dataset.\n",
    "\n",
    "- Cost Matrix:  A matrix that assigns a cost to each cell in the confusion matrix.\n",
    "\n",
    "Conceptually, the cost of labeling an example incorrectly should always be greaterthan the cost of labeling it correctly\n",
    " - —The Foundations Of Cost-sensitive Learning, 2001\n",
    "\n",
    "\n",
    "The values of the cost matrix must be carefully defined.  Like the choice of error functionfor traditional machine learning models, the choice of costs or cost function will determine thequality and utility of the model that is fit on the training data.\n",
    "\n",
    "There are perhaps three main groups of cost-sensitive methods that are most relevant forimbalanced learning; they are:\n",
    "\n",
    "- Cost-Sensitive Resampling\n",
    "- Cost-Sensitive Algorithms\n",
    "- Cost-Sensitive Ensembles \n",
    "\n",
    "#### Cost-Sensitive Resampling\n",
    "\n",
    "Data sampling is a technique that can be used for cost-sensitive learning directly.  Instead ofsampling with a focus on balancing the skewed class distribution, the focus is on changing thecomposition of the training dataset to meet the expectations of the cost matrix.  This mightinvolve directly sampling the data distribution or using a method to weight examples in thedataset.  Such methods may be referred to as cost-proportionate weighing of the training datasetor cost-proportionate sampling.\n",
    "\n",
    "#### Cost-Sensitive Algorithms\n",
    "\n",
    "Machine learning algorithms are rarely developed specifically for cost-sensitive learning.  Instead,the wealth of existing machine learning algorithms can be modified to make use of the cost matrix.\n",
    "\n",
    "Many such algorithm-specific augmentations have been proposed for popular algorithms, like decision trees and support vector machine.\n",
    "\n",
    "The scikit-learn Python machine learning library provides examples of these cost-sensitive extensions via the classweight argument on the following classifiers:\n",
    "\n",
    "- SVC\n",
    "- DecisionTreeClassifier\n",
    "\n",
    "Another more general approach to modifying existing algorithms is to use the costs as apenalty for misclassification when the algorithms are trained.  Given that most machine learningalgorithms are trained to minimize error, cost for misclassification is added to the error or usedto weigh the error during the training process.\n",
    "\n",
    "he scikit-learn library provides examples of these cost-sensitiveextensions via theclassweightargument on the following classifiers:\n",
    "\n",
    "- LogisticRegression\n",
    "- RidgeClassifier\n",
    "\n",
    "The Keras Python Deep Learning library also provides access to this use of cost-sensitive augmentation for neural networks via the classweightargument on thefit()function whentraining models. \n",
    "\n",
    "#### Cost-Sensitive Ensembles\n",
    "\n",
    "pass\n",
    "\n",
    "\n",
    "### Cost-Sensitive Logistic Regression\n",
    "\n",
    "The weighting can penalize the model less for errors made on examples from the majority class and penalize the model more for errors made on examples from the minority class.  The result is a version of logistic regression that performs better on imbalanced classification tasks,generally referred to as cost-sensitive or weighted logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC AUC: 0.985\n"
     ]
    }
   ],
   "source": [
    "# fit a logistic regression model on an imbalanced classification dataset\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=2)\n",
    "# define model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Logistic Regression for Imbalanced Classification\n",
    "\n",
    "Logistic regression is an effective model for binary classification tasks, although by default, it is not effective at imbalanced classification.  Logistic regression can be modified to be better suited for imbalanced classification.  The coefficients of the logistic regression algorithm are fitusing an optimization algorithm that minimizes the negative log likelihood (loss) for the modelon the training dataset.\n",
    "\n",
    "minn∑i=1−(log(yhati)×yi+ log(1−yhati)×(1−yi))\n",
    "\n",
    "This involves the repeated use of the model to make predictions followed by an adaptation ofthe coefficients in a direction that reduces the loss of the model.  The calculation of the loss fora given set of coefficients can be modified to take the class balance into account.  By default, theerrors for each class may be considered to have the same weighting, say 1.0.  These weightingscan be adjusted based on the importance of each class.\n",
    "\n",
    "\n",
    "minn∑i=1−(w0×log(yhati)×yi+w1×log(1−yhati)×(1−yi)\n",
    "\n",
    "The weighting is applied to the loss so that smaller weight values result in a smaller error value, and in turn, less update to the model coefficients.  \n",
    "\n",
    "A larger weight value results in a larger error calculation, and in turn, more update to the model coefficients.\n",
    "\n",
    "#### Weighted Logistic Regression with Scikit-Learn\n",
    "\n",
    "TheLogisticRegressionclass  provides  theclassweightargument that can be specified as a model hyperparameter.  Theclassweightis a dictionarythat defines each class label (e.g.  0 and 1) and the weighting to apply in the calculation of the negative log likelihood when fitting the model. \n",
    "\n",
    "The class weighing can be defined multiple ways; for example:\n",
    "\n",
    "- Domain expertise, determined by talking to subject matter experts\n",
    "\n",
    "- Tuning, determined by a hyperparameter search such as a grid search.\n",
    "\n",
    "- Heuristic, specified using a general best practice\n",
    "\n",
    "A best practice for using the class weighting is to use the inverse of the class distribution present in the training dataset.  For example, the class distribution of the test dataset is a 1:100ratio for the minority class to the majority class.  The inversion of this ratio could be used with1 for the majority class and 100 for the minority class\n",
    "\n",
    "We can evaluate the logistic regression algorithm with a class weighting using the sameevaluation procedure defined in the previous section.  We would expect that the class-weightedversion of logistic regression to perform better than the standard version of logistic regressionwithout any class weighting. \n",
    "\n",
    "The mean ROC AUC score is reported, in this case showing a better score than the unweightedversion of logistic regression, 0.989 as compared to 0.985\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC AUC: 0.989\n"
     ]
    }
   ],
   "source": [
    "# weighted logistic regression model on an imbalanced classification dataset\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=2)\n",
    "# define model\n",
    "weights = {0:0.01, 1:1.0}\n",
    "model = LogisticRegression(solver='lbfgs', class_weight=weights)\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the default class balance directly with the LogisticRegression class by settingtheclassweightargument to‘balanced’.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC AUC: 0.989\n"
     ]
    }
   ],
   "source": [
    "# weighted logistic regression for class imbalance with heuristic weights\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=2)\n",
    "# define model seeting class weight to balanced \n",
    "model = LogisticRegression(solver='lbfgs', class_weight='balanced')\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Weighted Logistic Regression\n",
    "\n",
    "\n",
    "Using a class weighting that is the inverse ratio of the training data is just a heuristic.  It ispossible that better performance can be achieved with a different class weighting, and this toowill depend on the choice of performance metric used to evaluate the model.  In this section, wewill grid search a range of different class weightings for weighted logistic regression and discoverwhich results in the best ROC AUC score.\n",
    "\n",
    "\n",
    "In this case, we can see that the 1:100 majority to minority class weighting achieved thebest mean ROC score.  This matches the configuration for the general heuristic.  It might beinteresting to explore even more severe class weightings to see their effect on the mean ROCAUC score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.988943 using {'class_weight': {0: 1, 1: 100}}\n",
      "0.982148 (0.017020) with: {'class_weight': {0: 100, 1: 1}}\n",
      "0.983465 (0.015555) with: {'class_weight': {0: 10, 1: 1}}\n",
      "0.985242 (0.013456) with: {'class_weight': {0: 1, 1: 1}}\n",
      "0.987973 (0.009846) with: {'class_weight': {0: 1, 1: 10}}\n",
      "0.988943 (0.006354) with: {'class_weight': {0: 1, 1: 100}}\n"
     ]
    }
   ],
   "source": [
    "# grid search class weights with logistic regression for imbalance classification\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=2)\n",
    "# define model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "# define grid\n",
    "balance = [{0:100,1:1}, {0:10,1:1}, {0:1,1:1}, {0:1,1:10}, {0:1,1:100}]\n",
    "param_grid = dict(class_weight=balance)\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n",
    "# execute the grid search\n",
    "grid_result = grid.fit(X, y)\n",
    "# report the best configuration\n",
    "print('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_))\n",
    "# report all configurations\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print('%f (%f) with: %r' % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
