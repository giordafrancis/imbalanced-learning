{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampling\n",
    "\n",
    "Data  sampling  provides  a  collection  of  techniques  that  transform  a  training  dataset  inorder to balance or better balance the class distribution.  Once balanced, standard machine learning algorithms can be trained directly on the transformed dataset without any modification.This  allows  the  challenge  of  imbalanced  classification,  even  with  severely  imbalanced  class distributions, to be addressed with a data preparation method.\n",
    "\n",
    " In this tutorial, you will discover a suite of data sampling techniques that can be used to balance an imbalanced classification dataset.  After completing this tutorial, you willknow:\n",
    " \n",
    " - The challenge of machine learning with imbalanced classification datasets.\n",
    " - The balancing of skewed class distributions using data sampling techniques.\n",
    " - Tour of data sampling methods for oversampling, undersampling, and combinations ofmethods\n",
    " \n",
    "### Problem with Imbalance learning\n",
    "\n",
    "*The hitch with imbalanced datasets is that standard classification learning algorithms are often biased towards the majority classes (known as “negative”) and therefore there is a higher misclassification rate in the minority class instances (called the “positive” class).*— Page 79,Learning from Imbalanced Data Sets, 2018\n",
    "\n",
    "### Balance the Class Distribution with sampling\n",
    "\n",
    "The reason that sampling methods are so common is because they are simple to understandand implement, and because once applied to transform the training dataset, a suite of standard machine learning algorithms can then be used directly.  This means that any from tens or hundreds of machine learning algorithms developed for balanced (or mostly balanced) classification can then be fit on the training dataset without any modification adapting them for the imbalance in observations.\n",
    "\n",
    "*Some models use prior probabilities, such as naive Bayes and discriminant analysisclassifiers.  Unless specified manually, these models typically derive the value of thepriors from the training data.  Using more balanced priors or a balanced training setmay help deal with a class imbalance.*— Page 426,Applied Predictive Modeling, 2013\n",
    "\n",
    "Sampling is only performed on the training dataset, the dataset used by an algorithm to learn a model.  It is not performed on the holdout test or validation dataset.  The reason is that the intent is not to remove the class bias from the model fit but to continue to evaluate the resulting model on data that is both real and representative of the target problem domain.As such, we can think of data sampling methods as addressing the problem of relative class imbalanced in the training dataset, and ignoring the underlying cause of the imbalance in the problem domain.  This is the difference between so-called relative and absolute rarity of examples in a minority class.\n",
    "\n",
    "There are two main types of data sampling used on the training dataset:  oversampling and undersampling.  In the next section, we will take a tour of popular methods from each type, as well as methods that combine multiple approaches.\n",
    "\n",
    "The following sections review some of the more popular methods, described in the context of binary (two-class) classification problems, which is a common practice, although most can be used directly or adapted for imbalanced classification with more than two classes.  The list here is based mostly on the approaches available in the scikit-learn friendly library, called imbalanced-learn.  For a longer list of data sampling methods, see Chapter 5 Data Level Preprocessing Methods in the 2018 book Learning from Imbalanced Data Sets.\n",
    "\n",
    "### Oversampling Techniques\n",
    "\n",
    "- Random Oversampling\n",
    "- Synthetic Minority Oversampling Technique (SMOTE)\n",
    "- Borderline-SMOTE\n",
    "- Borderline Oversampling with SVM\n",
    "- Adaptive Synthetic Sampling (ADASYN)\n",
    "\n",
    "\n",
    " The  simplest  oversampling  method  involves randomly duplicating examples from the minority class in the training dataset, referred to as Random Oversampling.  \n",
    "\n",
    "The most popular and perhaps most successful oversampling method is SMOTE; that is an acronym for Synthetic Minority Oversampling Technique.  SMOTE worksb y selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample as a point along that line.There are many extensions to the SMOTE method that aim to be more selective for the types  of  examples  in  the  minority  class  that  are  synthesized.   \n",
    "\n",
    "Borderline-SMOTE  involves selecting those instances of the minority class that are misclassified, such as with a k-nearest neighbor classification model, and only generating synthetic samples that are difficult to classify.\n",
    "\n",
    "Borderline Oversampling is an extension to SMOTE that fits an SVM to the dataset and uses the decision boundary as defined by the support vectors as the basis for generating synthetic examples, again based on the idea that the decision boundary is the area where more minority examples are required.\n",
    "\n",
    "Adaptive Synthetic Sampling (ADASYN) is another extension to SMOTE that generates synthetic samples inversely proportional to the density of the examples in the minority class.  It is designed to create synthetic examples in regions of the feature space where the density of minority examples is low, and fewer or none where the density is high.  For more on oversampling methods, see Chapter 12.\n",
    "\n",
    "###  Undersampling Techniques\n",
    "\n",
    "- Random Undersampling\n",
    "- Condensed Nearest Neighbor Rule (CNN)\n",
    "- Near Miss Undersampling\n",
    "- Tomek Links Undersampling\n",
    "- Edited Nearest Neighbors Rule (ENN)\n",
    "- One-Sided Selection (OSS)\n",
    "- Neighborhood Cleaning Rule (NCR)\n",
    "\n",
    " The simplest undersampling method involves randomly deleting examples from the majority class in the training dataset,  referred to as random undersampling.  One group of techniques involves selecting a robust and representative subset of the examples in the majority class.\n",
    " \n",
    "The Condensed Nearest Neighbors rule, or CNN for short, was designed for reducing the memory required for the k-nearest neighbors algorithm.It works by enumerating the examples in the dataset and adding them to the store only if they cannot be classified correctly by the current contents of the store, and can be applied to reduce the number of examples in the majority class after all examples in the minority class have been added to the store.\n",
    "\n",
    "Near Miss refers to a family of methods that use KNN to select examples from the majority class.   NearMiss-1 selects examples from the majority class that have the smallest average distance to the three closest examples from the minority class.  NearMiss-2 selects examples from the majority class that have the smallest average distance to the three furthest examples fromthe minority class.  NearMiss-3 involves selecting a given number of majority class examples for each example in the minority class that are closest.\n",
    "\n",
    "Another group of techniques involves selecting examples from the majority class to delete.These approaches typically involve identifying those examples that are challenging to classify and therefore add ambiguity to the decision boundary.  Perhaps the most widely known deletion undersampling  approach  is  referred  to  as  Tomek  Links,  originally  developed  as  part  of  an extension to the Condensed Nearest Neighbors rule.  A Tomek Link refers to a pair of examples in the training dataset that are both nearest neighbors (have the minimum distance in feature space) and belong to different classes.  **Tomek Links are often misclassified examples found along the class boundary and the examples in the majority class are deleted.**\n",
    "\n",
    "The  Edited  Nearest  Neighbors  rule,  or  ENN  for  short,  is  another  method  for  selecting examples for deletion.  This rule involves using k= 3 nearest neighbors to locate those examples in a dataset that are misclassified and deleting them.  The ENN procedure can be repeated multiple times on the same dataset, better refining the selection of examples in the majority class.  This extension is referred to initially as unlimited editing although it is more commonly referred to as Repeatedly Edited Nearest Neighbors.  Staying with the select to keepv s.select to delete families of undersampling methods, there are also undersampling methods that combine both approaches.\n",
    "\n",
    "One-Sided Selection, or OSS for short, is an undersampling technique combines Tomek Links and the Condensed Nearest Neighbor (CNN) Rule.  The Tomek Links method is used to remove noisy examples on the class boundary, whereas CNN is used to remove redundant examples from the interior of the density of the majority class.  The Neighborhood Cleaning Rule, or NCRfor short, is another combination undersampling technique that combines both the Condensed Nearest Neighbor (CNN) Rule to remove redundant examples and the Edited Nearest Neighbors(ENN) Rule to remove noisy or ambiguous examples. \n",
    "\n",
    "### Combination of techniques\n",
    "\n",
    "Experiments have shown that applying both types of techniques together can often result in better overall performance of a model fit on the resulting transformed dataset.  Some of the more widely used and implemented combinations of data sampling methods include:\n",
    "\n",
    "- SMOTE and Random Undersampling\n",
    "- SMOTE and Tomek Links\n",
    "- SMOTE and Edited Nearest Neighbors Rule\n",
    "\n",
    "\n",
    " SMOTE is perhaps the most popular and widely used oversampling technique.  It is common to pair SMOTE with an undersampling method that selects examples from the dataset to delete, and the procedure is applied to the dataset. after SMOTE, allowing the editing step to be applied to both the minority and majority class.\n",
    "The intent is to remove noisy points along the class boundary from both classes, which seems to have the effect of the better performance of classifiers fit on the transformed dataset.\n",
    "\n",
    "Two popular examples involve using SMOTE followed by the deletion of Tomek Links, and SMOTE followed by the deletion of those examples misclassified via a KNN model, the so-called Edited Nearest Neighbors rule.  For more on combining oversampling and under sampling methods, see Chapter 14.\n",
    "\n",
    "### Random Oversampling\n",
    "\n",
    "Random oversampling involves randomly duplicating examples from the minority class and  adding them to the training dataset.  Examples from the training dataset are selected randomly with replacement.  This means that examples from the minority class can be chosen and added to the new more balanced training dataset multiple times; they are selected from the original training dataset, added to the new training dataset, and then returned or replaced in the original dataset, allowing them to be selected again.\n",
    "\n",
    "\n",
    "This technique can be effective for those machine learning algorithms that are affected by a skewed distribution and where multiple duplicate examples for a given class can influence the fit of the model.  This might include algorithms that iteratively learn coefficients, like artificialneural networks that use stochastic gradient descent.  It can also affect models that seek goodsplits of the data, such as support vector machines and decision trees.\n",
    "\n",
    "It might be useful to tune the target class distribution.  In some cases, seeking a balanced distribution  for  a  severely  imbalanced  dataset  can  cause  affected  algorithms  to  overfit  the minority class, leading to increased generalization error.  The effect can be better performance on the training dataset, but worse performance on the holdout or test dataset.\n",
    "\n",
    "The increase in the number of examples for the minority class, especially if the class skew was severe, can also result in a marked increase in the computational cost when fitting the model, especially considering the model is seeing thesame examples in the training dataset again and again\n",
    "\n",
    "As such, to gain insight into the impact of the method, it is a good idea to monitor the performance on both train and test datasets after oversampling and compare the results to the same algorithm on the original dataset.  \n",
    "\n",
    "Random oversampling can be implemented using the `RandomOverSamplerclass`.  The class can be defined and takes a sampling strategy argument that can be set to ‘minority’ to automatically balance the minority class with majority class or classes. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original Counter({0: 9900, 1: 100})\n",
      "equal Counter({0: 9900, 1: 9900})\n",
      "50pc Counter({0: 9900, 1: 4950})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "X, y = make_classification(n_samples=10_000, weights=[0.99], flip_y=0)\n",
    "print('original', Counter(y))\n",
    "\n",
    "# define oversampling strategy\n",
    "oversample_eq = RandomOverSampler(sampling_strategy='minority')\n",
    "oversample_mid = RandomOverSampler(sampling_strategy=0.5)\n",
    "\n",
    "# fit and apply the transfor\n",
    "X_over, y_over = oversample_eq.fit_resample(X, y)\n",
    "print('equal', Counter(y_over))\n",
    "X_over, y_over = oversample_mid.fit_resample(X, y)\n",
    "print('50pc', Counter(y_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transform can be used as part of a Pipeline to ensure that it is only applied to the training dataset as part of each split in a k-fold cross-validation.  A traditional scikit-learn Pipeline cannot be used; instead, a Pipeline from the imbalanced-learn library can be used.\n",
    "\n",
    " Your specific results may differ given the stochastic nature of the dataset and the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f-measure 0.9806666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "random_state = np.random.RandomState(seed= 1)\n",
    "\n",
    "X, y = make_classification(n_samples=1000, weights=[0.99], flip_y=0, random_state=random_state)\n",
    "# define pipeline\n",
    "\n",
    "steps = [\n",
    "    ('over', RandomOverSampler(sampling_strategy='minority', random_state=random_state)),\n",
    "    ('model', DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "kfold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, \n",
    "                                random_state=random_state)\n",
    "\n",
    "scores = cross_val_score(estimator= pipeline, cv = kfold, n_jobs=-1,\n",
    "                         X=X, y=y, scoring='f1_micro')\n",
    "\n",
    "print('f-measure', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Undersampling\n",
    "\n",
    "Random undersampling involves randomly selecting examples from the majority class to delete from the training dataset.  This has the effect of reducing the number of examples in the majority class in the transformed version of the training dataset.  This process can be repeated until the desired class distribution is achieved, such as an equal number of examples for each class.\n",
    "\n",
    "**This approach may be more suitable for those datasets where there is a class imbalance although still a sufficient number of examples in the minority class, such that a useful model can be fit.**\n",
    "\n",
    "A limitation of undersampling is that examples from the majority class are deleted that may be useful, important, or perhaps critical to fitting a robust decision boundary.  Given that  examples  are  deleted  randomly,  there  is  no  way  to  detect  or  preserve good or  more information-rich examples from the majority class.\n",
    "\n",
    "\n",
    "The random undersampling technique can be implemented using the `RandomUnderSampler` imbalanced-learn class.  The class can be used just like the `RandomOverSampler` class in the previous section, except the strategies impact the majority class instead of the minority class.For example, setting the sampling strategy argument to ‘majority’ will undersample the majority class determined by the class with the largest number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 9900, 1: 100})\n",
      "Counter({0: 100, 1: 100})\n"
     ]
    }
   ],
   "source": [
    "# example of random undersampling to balance the class distribution\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, weights=[0.99], flip_y=0)\n",
    "# summarize class distribution\n",
    "print(Counter(y))\n",
    "# define undersample strategy\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "# fit and apply the transform\n",
    "X_over, y_over = undersample.fit_resample(X, y)\n",
    "# summarize class distribution\n",
    "print(Counter(y_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Synthetic Minority Oversampling Technique\n",
    "\n",
    "An improvement on duplicating examples from the minority class is to synthesize new examples from the minority class.  This is a type of data augmentation for tabular data and can be very effective.\n",
    "\n",
    "Perhaps the most widely used approach to synthesizing new examples is called the SyntheticMinority Oversampling TEchnique, or SMOTE for short.  This technique was described by Nitesh Chawla, et al.  in their 2002 paper named for the technique titled SMOTE: Synthetic Minority Over-sampling Technique.  SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n",
    "\n",
    "Specifically, a random example from the minority class is first chosen.  Then k of the nearest neighbors for that example are found (typically k= 5).  A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.\n",
    "\n",
    "This procedure can be used to create as many synthetic examples for the minority class as are required.  As described in the paper, it suggests first using random undersampling to trim the number of examples in the majority class, then use SMOTE to oversample the minority class to balance the class distribution.\n",
    "\n",
    "The approach is effective because new synthetic examples from the minority class are created that are plausible, that is, are relatively close in feature space to existing examples from the minority class.\n",
    "\n",
    "A general downside of the approach is that synthetic examples are created without considering the majority class, possibly resulting in ambiguous examples if there is a strong overlap for the classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
